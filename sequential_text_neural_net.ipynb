{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab6SequentialText.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "clp2XXLYovB1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 6: Sequence to Sequence Text"
      ]
    },
    {
      "metadata": {
        "id": "rE-PqK44pBRQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Import Statements"
      ]
    },
    {
      "metadata": {
        "id": "7Z_RUl85t-rJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "08db7f32-8492-4f15-afd1-2b9364993e3d"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch \n",
        "!pip3 install tqdm\n",
        "!wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "!tar -xzf text_files.tar.gz\n",
        "!pip3 install unidecode"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.26.0)\n",
            "--2018-10-13 21:06:02--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 34.200.202.18, 34.237.217.71, 52.20.136.189, ...\n",
            "Connecting to piazza.com (piazza.com)|34.200.202.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2018-10-13 21:06:02--  https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)... 13.33.164.102, 13.33.164.12, 13.33.164.9, ...\n",
            "Connecting to d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)|13.33.164.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2018-10-13 21:06:02 (22.0 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.0.22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AiYeLs4h0Gkq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "from torch.autograd import Variable\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6wB2Q3ckpxll",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Functions used by the net"
      ]
    },
    {
      "metadata": {
        "id": "-KpTc3ZBv-_S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "     tensor[c] = all_characters.index(string[c])\n",
        "  return Variable(tensor)\n",
        "  \n",
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "93os7_rPz5Ao",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(inp, target):\n",
        "  ## initialize hidden layers, set up gradient and loss \n",
        "  # your code here\n",
        "  hidden = decoder.init_hidden() #initialize hidden\n",
        "  decoder_optimizer.zero_grad()\n",
        "  loss = 0\n",
        "  for c in range(chunk_len):\n",
        "    output, hidden = decoder(inp[c], hidden) # run the forward pass of your rnn with proper input\n",
        "    loss += criterion(output, target[c].unsqueeze(0))\n",
        "\n",
        "  ## calculate backwards loss and step the optimizer (globally)\n",
        "  # your code here\n",
        "  ## /\n",
        "  \n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.item() / chunk_len\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  ## initialize hidden variable, initialize other useful variables \n",
        "  hidden = decoder.init_hidden() #initialize hidden\n",
        "  predicted = prime_str #initialize predicted \n",
        "  ## /\n",
        "\n",
        "  prime_input = char_tensor(prime_str)\n",
        "  \n",
        "  # Use priming string to \"build up\" hidden state\n",
        "  for p in range(len(prime_str) - 1):\n",
        "      _, hidden = decoder(prime_input[p], hidden)\n",
        "  inp = prime_input[-1] #This is an index, not words\n",
        "\n",
        "  for p in range(predict_len): #Makes the hidden layer for large strings better, then use your input to predict on.\n",
        "      #Same as taking last letter of prime string if prime str is single\n",
        "      output, hidden = decoder(inp, hidden) #run your RNN/decoder forward on the input\n",
        "\n",
        "      # Sample from the network as a multinomial distribution\n",
        "      output_dist = output.data.view(-1).div(temperature).exp()\n",
        "      top_i = torch.multinomial(output_dist, 1)[0] #gives you the index of where the most\n",
        "\n",
        "      ## get character from your list of all characters, add it to your output str sequence, set input\n",
        "      ## for the next pass through the model\n",
        "      predicted += all_characters[top_i.item()]  #update predicted\n",
        "      inp = top_i\n",
        "      ## /\n",
        "\n",
        "  return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fh1FELsqvEx1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The GRU and RNN"
      ]
    },
    {
      "metadata": {
        "id": "l5hA_l-C4KbY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, n_layers=1 ):\n",
        "    super(GRU, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    self.r_t = nn.Sequential(nn.Linear(self.input_size + self.hidden_size, self.hidden_size, bias = True), nn.Sigmoid())\n",
        "    self.z_t = nn.Sequential(nn.Linear(self.input_size + self.hidden_size, self.hidden_size, bias = True), nn.Sigmoid())\n",
        "    self.n_t = nn.Sequential(nn.Linear(self.input_size + self.hidden_size, self.hidden_size, bias = True), nn.Tanh())\n",
        "    \n",
        "  def forward(self, input, hidden):\n",
        "    t = torch.cat((input, hidden), dim = 2) #What is the dimension of W? I.E. what are we concatenating\n",
        "    r = self.r_t(t)\n",
        "    z = self.z_t(t)\n",
        "    \n",
        "    rh = torch.cat((input, hidden), dim = 2)\n",
        "    n = self.n_t(rh)\n",
        "    h = (1-z)*n + z*hidden\n",
        "    \n",
        "    return h,h\n",
        "  \n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # encode using embedding layer\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    # set up GRU passing in number of layers parameter (nn.GRU)\n",
        "    self.gru = GRU(input_size, hidden_size)\n",
        "    # decode output\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "    # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "    # of the GRU\n",
        "    # return output and hidden\n",
        "    output = self.embedding(input_char).view(1, 1, -1)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.softmax(self.out(output[0]))\n",
        "    return output, hidden\n",
        "      \n",
        "  def init_hidden(self):\n",
        "    return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xn6x01L4wsrW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Runs of the Model\n",
        " - note that the spec file requests 15 samples per dataset\n",
        " - I ran the model on the given LOTR, Alma, and Shakespeare data set, as well as a dataset containing some of Arthur Conan Doyles Sherlock Holmes stories"
      ]
    },
    {
      "metadata": {
        "id": "HtVO2SkTvcM0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lord of The Rings"
      ]
    },
    {
      "metadata": {
        "id": "NqdzYh3jEbE7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "e858b529-af42-452e-bf7f-8c556e71dc34"
      },
      "cell_type": "code",
      "source": [
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        " \n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "\n",
        "chunk_len = 200\n",
        "print(random_chunk())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 2579888\n",
            "a deep dike with a steep wall on the further \n",
            "side. Tom said that it had once been the boundary of a kingdom, but a very \n",
            "long lime ago. He seemed to remember something sad about it, and would not \n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zkzf7DIVBbG5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "ab41ed11-40cd-4cd6-e89d-14304dfb9922"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "n_epochs = 2000\n",
        "print_every = 1000\n",
        "plot_every = 10\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        " \n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "    print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "    print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "    all_losses.append(loss_avg / plot_every)\n",
        "    loss_avg = 0"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[175.41901993751526 (1000 50%) 1.9932]\n",
            "Whinh as wither long gried and mon to me Meres in hirses Rishber thither. Eanter, a foun of a das was  \n",
            "\n",
            "[351.6507213115692 (2000 100%) 1.6543]\n",
            "Whiled the grows be me to in the last \n",
            "leath. And stiless the sanishave \n",
            "fross to to with then \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hiFHVs28wkwb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Alma"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ZMc2XpED-mpq",
        "outputId": "81f2ddb8-528a-4c63-ca40-3e1c7c8a1de3"
      },
      "cell_type": "code",
      "source": [
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        " \n",
        "file = unidecode.unidecode(open('./text_files/alma.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "\n",
        "chunk_len = 200\n",
        "print(random_chunk())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 466656\n",
            "to him and his brethren.\n",
            "\n",
            " And now, these are the words of Ammon to his brethren, which say thus: My brothers and my brethren, behold I say unto you, how great reason have we to rejoice; for could we h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fubuFL0QxcL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "8350fa0f-30d3-44b0-ba64-e55fe4f1867d"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "n_epochs = 2000\n",
        "print_every = 130\n",
        "plot_every = 10\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        " \n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "    print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "    print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "    all_losses.append(loss_avg / plot_every)\n",
        "    loss_avg = 0"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[22.865074396133423 (130 6%) 2.2873]\n",
            "Wh wmaga, fot one chith be pen an ith cow uthirte and ther ake houl of wile laor i ind cond; wo matave \n",
            "\n",
            "[45.68302130699158 (260 13%) 2.0276]\n",
            "Wh.\n",
            "\n",
            " whwren in thowl veime helould to gith and in nou and uter nt has and in singed gironto Gong if t \n",
            "\n",
            "[68.48992896080017 (390 19%) 1.4951]\n",
            "Whare now or wereo igtore all beane greoul of on ave ceire iz behe caperen ood ous be worlsed the cass \n",
            "\n",
            "[91.40008878707886 (520 26%) 1.6707]\n",
            "Wh and for it sporbe at or allie.\n",
            "\n",
            " reptle and the mich intholige the dy mile the a como or the Noured \n",
            "\n",
            "[114.28969955444336 (650 32%) 1.4352]\n",
            "Whing that ye land he dits reminittert prongoran they el ye to that land the man to fore, and the come \n",
            "\n",
            "[137.09772658348083 (780 39%) 1.2309]\n",
            "Whoreven, and be prelen ye came that thee bristenest you, land of strands, and to cossied yout Pore to \n",
            "\n",
            "[160.21630382537842 (910 45%) 1.8157]\n",
            "Wh, in shallen they hass sain ened, them beeo ard stard, that they had prile sur aganing he will with  \n",
            "\n",
            "[183.40703988075256 (1040 52%) 1.3666]\n",
            "Whord, and for the Land again agan their His of which I I they recend; and a shir of God bleseever of  \n",
            "\n",
            "[206.27515602111816 (1170 58%) 1.4315]\n",
            "Whired to courverp and it suppain of Chaturn his cepale into Amcitise that serevelice and he de that g \n",
            "\n",
            "[229.20996117591858 (1300 65%) 1.6579]\n",
            "Wh your it his hearms--Therted--The wase to camm our nith seid of these ye or in the to was touress--P \n",
            "\n",
            "[252.03717303276062 (1430 71%) 1.7856]\n",
            "Wher dive deaster us war delitions of Moreening concered to people ustiont or murnge to Morony his bli \n",
            "\n",
            "[274.8873977661133 (1560 78%) 1.6129]\n",
            "Whould that he his with he will of the Lord with his were himber; the plaven and to pas untion, and lo \n",
            "\n",
            "[297.73033356666565 (1690 84%) 1.8824]\n",
            "Whores two of the suppess, and the land; and thirnessing.\n",
            "\n",
            " And father as camen to perchice to prople; \n",
            "\n",
            "[320.63404870033264 (1820 91%) 1.4468]\n",
            "Wh the for from which pass this sppose they the should in God, that the serce upne of that our lahouch \n",
            "\n",
            "[343.5237514972687 (1950 97%) 1.2107]\n",
            "Whred unto preedness I vood and, and amoy ame torest in the land dave nuccess, and could is not should \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tIBV7XVHxPQk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Shakespeare"
      ]
    },
    {
      "metadata": {
        "id": "YzfYAN8quypr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c19993d9-0787-428f-90b0-c46265ca1bc2"
      },
      "cell_type": "code",
      "source": [
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        " \n",
        "file = unidecode.unidecode(open('./text_files/tiny_shakespeare.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "\n",
        "chunk_len = 500\n",
        "print(random_chunk())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 1115394\n",
            " is dead.\n",
            "The king my uncle is to blame for this:\n",
            "God will revenge it; whom I will importune\n",
            "With daily prayers all to that effect.\n",
            "\n",
            "Girl:\n",
            "And so will I.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Peace, children, peace! the k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JZhJFuP0xmOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "edfee54c-7ffd-4426-ac4e-d8057be51557"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "n_epochs = 2000\n",
        "print_every = 130\n",
        "plot_every = 10\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        " \n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "    print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "    print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "    all_losses.append(loss_avg / plot_every)\n",
        "    loss_avg = 0"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[90.71150422096252 (500 25%) 2.0692]\n",
            "Wher to the in fith blour migher,\n",
            "Af that it lou of wer eace cold shiclon,\n",
            "As you mo,\n",
            "An's arcate my e \n",
            "\n",
            "[181.56948494911194 (1000 50%) 1.8662]\n",
            "Whart!\n",
            "Of plander prold, semay, be one frar,\n",
            "We a dower lit:\n",
            "And wast sountion the sunter the deare id \n",
            "\n",
            "[272.5564832687378 (1500 75%) 1.7651]\n",
            "Wher; genent loods\n",
            "Whought the doth shich, and many hen the plaith\n",
            "O, the hat, grablers the plight pes \n",
            "\n",
            "[363.2676250934601 (2000 100%) 1.8596]\n",
            "What Luave san, well whuble and at\n",
            "sispit and it him, his not stood castrent his hither.\n",
            "Loven of not  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6oaZlNkdw-l0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sherlock Holmes"
      ]
    },
    {
      "metadata": {
        "id": "W-kFx2vhxBfF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0ecf61f6-9830-49b9-ba3d-baa7a59e4b2a"
      },
      "cell_type": "code",
      "source": [
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        " \n",
        "file = unidecode.unidecode(open('./sherlock_holmes.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "\n",
        "chunk_len = 500\n",
        "print(random_chunk())"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 143124\n",
            "ith his knife, and I had to grasp him twice, and got a cut over the knuckles, before I had the upper hand of him. He looked murder out of the only eye he could see with when we had finished, but he listened to reason and gave up the papers. Having got them I let my man go, but I wired full particulars to Forbes this morning. If he is quick enough to catch his bird, well and good. But if, as I shrewdly suspect, he finds the nest empty before he gets there, why, all the better for the government. I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wYo_8HO_xy8Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "b03bea79-4edc-4aca-c587-bb5310f63523"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "n_epochs = 2000\n",
        "print_every = 130\n",
        "plot_every = 10\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        " \n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "    print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "    print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "    all_losses.append(loss_avg / plot_every)\n",
        "    loss_avg = 0"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[61.55309176445007 (130 6%) 2.2109]\n",
            "Whe isllfonded I and the tolkat at behe wimt heon I andang sothe werctouc a he geasgat whersen I youve \n",
            "\n",
            "[123.96553087234497 (260 13%) 2.0102]\n",
            "Whering and the beas ther the Jatticticher sode you save sine, I the reer ary ailr mured bave yoq to m \n",
            "\n",
            "[185.7848024368286 (390 19%) 1.9695]\n",
            "Whon Ally. A hadiccel and bour heaking a purted the ordenglathed beaded and and the and whof one and w \n",
            "\n",
            "[247.6882860660553 (520 26%) 1.9110]\n",
            "Wh was cank, sept. Hoom, anf am is a proffling of his my a star, and his iver expon my smerpion my sav \n",
            "\n",
            "[309.5395288467407 (650 32%) 1.8600]\n",
            "Why the done nos wabed whow clintied of the wait that. Hor he lun with we the co moveay this the net h \n",
            "\n",
            "[371.4916527271271 (780 39%) 1.7035]\n",
            "Wher, I where beep his cluble me back and Myserply the ofpaned My me thich had everpensice had I help  \n",
            "\n",
            "[433.74994349479675 (910 45%) 1.7528]\n",
            "Whare I his over you sepped which shost in to lidly of beer he had storn of a raid. I worched that in  \n",
            "\n",
            "[495.64603638648987 (1040 52%) 1.8721]\n",
            "Whe stroughed mowmed ary the thou know windon.\n",
            "\n",
            "Wh in the courwul the some to well you him to my for a \n",
            "\n",
            "[557.3564805984497 (1170 58%) 1.8597]\n",
            "Whard have mation, a stame in a care in hure, I have doon had training would dairs. Do dooved thar rem \n",
            "\n",
            "[619.2124691009521 (1300 65%) 1.6976]\n",
            "Whated to to have recand in and had be word of and from that he dound and Nacty and has wellwaking hea \n",
            "\n",
            "[680.983229637146 (1430 71%) 1.5790]\n",
            "Whare I had bettal you with wore the to the wo that which hapectent.\n",
            "\n",
            "I remiciciated that indlervy you \n",
            "\n",
            "[743.0905792713165 (1560 78%) 1.6883]\n",
            "Whered lust dank at yet is scind would be a into at lout mounge stan was me this compreist think gas a \n",
            "\n",
            "[805.0622172355652 (1690 84%) 1.6463]\n",
            "Wher he have in Lout for in the lanch prtortion the conter acqual is was jouet Strensobled poge in the \n",
            "\n",
            "[866.8030259609222 (1820 91%) 1.6765]\n",
            "Wh soming of me. The intracts and as his noth the arther, de. be left his somed and I gawing of a have \n",
            "\n",
            "[928.6851568222046 (1950 97%) 1.6653]\n",
            "Wher asked, and he we dikenitt others a late dose reokin were would leathingy told she fate where very \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hWQNCtMBQhar",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}